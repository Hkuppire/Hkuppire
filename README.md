## Hi there ğŸ‘‹
# I'm Hema Priya Kuppireddy ğŸ‘‹  
### Data Engineer | Python Developer | Cloud & Big Data Enthusiast

I enjoy turning raw data into reliable, scalable pipelines using modern data engineering tools.  
My work spans ETL/ELT workflows, cloud data platforms, SQL optimization, and distributed processing with PySpark.

Iâ€™m always learning, building, and exploring new ways to make data cleaner, faster, and more useful.

---

## ğŸš€ Tech Stack

### ğŸŸ¦ Languages  
- **Python**, SQL, PySpark  
- Bash scripting  

### â˜ï¸ Cloud & Data Platforms  
- **Google Cloud** (BigQuery, Dataflow, Composer)  
- **AWS** (Lambda, Glue, S3, Redshift)  
- **Azure** (ADF, Synapse, Key Vault)

### ğŸ›  Data Engineering Tools  
- Apache Airflow  
- dbt  
- Docker  
- APIs & Automation  
- Parquet, Delta Lake, JSON, CSV  

### ğŸ“Š Databases  
- BigQuery  
- Snowflake  
- Redshift  
- PostgreSQL  
- SQL Server  

### ğŸ“ˆ Analytics & ML  
- Pandas, NumPy  
- Scikit-learn  
- Exploratory Data Analysis  
- Feature engineering  

---

## ğŸ“š Featured Projects

### ğŸ”¹ **SQL Portfolio**  
Advanced SQL queries showcasing joins, window functions, CTEs, retention logic, and analytics.  
ğŸ”— https://github.com/Hkuppire/sql-portfolio

### ğŸ”¹ **Python ETL Pipeline**  
A simple, production-style ETL pipeline that extracts CSV, transforms it with Pandas, and loads to Parquet.  
ğŸ”— https://github.com/Hkuppire/python-etl-sample

### ğŸ”¹ **Airflow ETL DAG**  
A basic daily ETL DAG demonstrating Airflow orchestration using PythonOperators.  
ğŸ”— https://github.com/Hkuppire/airflow-dag-example

### ğŸ”¹ **PySpark Data Transformation**  
PySpark workflow for cleaning, transforming, and writing large datasets to Parquet.  
ğŸ”— https://github.com/Hkuppire/pyspark-transformation

---

## ğŸŒ± What I'm Learning Now  
- Improving PySpark optimization  
- Advanced dbt modeling  
- Cloud data architectures (GCP & Azure)  
- ML workflow automation  

---

## ğŸ“« Connect With Me  
ğŸ“ United States  
ğŸ“§ **khemapriya214@gmail.com**  

---

â­ *Thanks for visiting! Feel free to explore my repos or reach out â€” always happy to collaborate or discuss data engineering.*

